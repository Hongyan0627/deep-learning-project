{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###PLEASE NOTE\n",
    "##we used some of the data loading/saving code from here:\n",
    "##Fierro, Miguel, Ye Xing, and Tao Wu. \"Quick-Start Guide to the Data Science Bowl Lung Cancer Detection Challenge, Using Deep Learning, Microsoft Cognitive Toolkit and Azure GPU VMs.\" Blog post. Cortana Intelligence and Machine Learning Blog. Microsoft, 17 Feb. 2017. Web. 10 Mar. 2017.\n",
    "##this is the short code to load the features from file and make an array per patient\n",
    "\n",
    "\n",
    "\n",
    "devtype = 1 #1 for gpu, 0 for cpu\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs,\\\n",
    "        INFINITELY_REPEAT\n",
    "from cntk.learners import sgd, learning_rate_schedule, UnitType\n",
    "from cntk import input, cross_entropy_with_softmax, \\\n",
    "        classification_error, sequence\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.layers import Sequential, Embedding, Recurrence, LSTM, GRU, Dense, \\\n",
    "        BatchNormalization, Dropout, For, Convolution, Fold\n",
    "from cntk.io import UserMinibatchSource, StreamInformation, MinibatchData\n",
    "from cntk.ops.functions import Function\n",
    "\n",
    "import cntk as C\n",
    "\n",
    "from __future__ import print_function\n",
    "from cntk.layers import *\n",
    "from cntk.layers.sequence import *\n",
    "from cntk.layers.models.attention import *\n",
    "from cntk.layers.typing import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if devtype ==1:\n",
    "    C.device.try_set_default_device(C.device.gpu(0))\n",
    "else:\n",
    "    C.device.try_set_default_device(C.device.cpu())\n",
    "\n",
    "\n",
    "\n",
    "##this grouper function is just from SO\n",
    "def grouper(n, iterable):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NUMBER = '001' \n",
    "\n",
    "#Put here the path where you downloaded all kaggle data\n",
    "DATA_PATH='/a/h/cole/data/'\n",
    "\n",
    "# Path and variables\n",
    "STAGE1_LABELS=DATA_PATH + 'stage1_labels.csv'\n",
    "STAGE1_SAMPLE_SUBMISSION=DATA_PATH + 'stage1_sample_submission.csv'\n",
    "STAGE1_SAMPLE_SUBMISSION_SOLUTIONS=DATA_PATH + 'stage1_solution.csv'\n",
    "STAGE1_FOLDER=DATA_PATH + 'stage1/'\n",
    "FEATURE_FOLDER=DATA_PATH + 'features/features' + EXPERIMENT_NUMBER + '/'\n",
    "SUBMIT_OUTPUT='submit' + EXPERIMENT_NUMBER + '.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_two_classes(y):\n",
    "    ys=[]\n",
    "    for label in y:\n",
    "        if label:\n",
    "            ys.append((1,0))\n",
    "        else:\n",
    "            ys.append((0,1))\n",
    "    return np.array(ys).astype(np.float32)\n",
    "\n",
    "df = pd.read_csv(STAGE1_LABELS)\n",
    "\n",
    "# x = [np.squeeze(np.load(FEATURE_FOLDER+'%s.npy' % str(id))) for id in df['id'].tolist()]\n",
    "\n",
    "xresnet = [np.squeeze(np.load(FEATURE_FOLDER+'%s.npy' % str(id))) for id in df['id'].tolist()]\n",
    "xvgg = [np.squeeze(np.load('/a/data/mango/featuresvgg19/'+'%s.npy' % str(id))) for id in df['id'].tolist()]\n",
    "\n",
    "x=[]\n",
    "for r,v in zip(xresnet,xvgg):\n",
    "    v = v.reshape((v.shape[0],-1))\n",
    "    x.append(np.concatenate((r,v),axis=1))\n",
    "\n",
    "\n",
    "#gives python list of length 1397, each item has sizes like (88, 2048) or (77, 2048)\n",
    "#x = np.array(x)\n",
    "y = df['cancer'].as_matrix()\n",
    "# print y.shape (1397,)\n",
    "\n",
    "#x_train, x_val, y_train, y_val = cross_validation.train_test_split(x, y, random_state=42, stratify=y,\n",
    "#                                                                       test_size=0.20)\n",
    "y = make_two_classes(y)\n",
    "\n",
    "# xm = []\n",
    "# for xi in x:\n",
    "#     xm.append(xi / xi.shape[0])\n",
    "    \n",
    "# x = xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle\n",
    "# # cPickle.dump(x, open('x_test_resvgg.pickle', 'wb')) \n",
    "# # cPickle.dump(y, open('y_test_resvgg.pickle', 'wb')) \n",
    "\n",
    "# x = cPickle.load(open('x_test_resvgg.p', 'rb'))\n",
    "# y = cPickle.load(open('y_test_resvgg.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = x[0].shape[1]\n",
    "\n",
    "\n",
    "#had to fight with cntk... custom batch source for our numpy data\n",
    "class MyDataSource(UserMinibatchSource):\n",
    "    def __init__(self, f_dim, l_dim):\n",
    "        #pass\n",
    "        self.f_dim, self.l_dim = f_dim, l_dim\n",
    "\n",
    "        self.fsi = StreamInformation(\"features\", 0, 'dense', np.float32, (self.f_dim,))\n",
    "        self.lsi = StreamInformation(\"labels\", 1, 'dense', np.float32, (self.l_dim,))\n",
    "\n",
    "#        self.data_iter = grouper(minibatch_size,x)\n",
    "#        self.label_iter = grouper(minibatch_size,y)\n",
    "        \n",
    "#         print(\"a\")\n",
    "        super(MyDataSource, self).__init__()\n",
    "\n",
    "    #this whole function is to work around a bug in cntk wtf\n",
    "    def turn_var_into_value_wtf(self,var,data):\n",
    "        data = [C.Value._as_best_data_type(var, sample) for sample in data]\n",
    "        borrow = devtype == C.DeviceKind.CPU\n",
    "        list_of_ndavs = [C.NDArrayView.from_data(sample, device=C.cpu(),\n",
    "                                               borrow=borrow)\n",
    "                         for sample in data]\n",
    "\n",
    "        from cntk.internal import sanitize_shape\n",
    "        value = C.cntk_py.Value_create(\n",
    "            sanitize_shape(var.shape),\n",
    "            list_of_ndavs,\n",
    "            [],\n",
    "            C.use_default_device(),\n",
    "            False,\n",
    "            True)  # always create a copy in Value\n",
    "        return value\n",
    "    \n",
    "    def stream_infos(self):\n",
    "#        pass\n",
    "#        print(\"b\")\n",
    "        return [self.fsi, self.lsi]\n",
    "\n",
    "    def next_minibatch(self, num_samples, number_of_workers=1, worker_rank=0, device=None):\n",
    "#        pass\n",
    "#        print(\"c\")\n",
    "#         try:\n",
    "#             train_features = list(next(self.data_iter))\n",
    "#             labels = list(next(self.label_iter))\n",
    "#         except StopIteration:\n",
    "#             #continue\n",
    "#             self.data_iter = grouper(minibatch_size,x)\n",
    "#             self.label_iter = grouper(minibatch_size,y)\n",
    "#             train_features = list(next(self.data_iter))\n",
    "#             labels = list(next(self.label_iter))\n",
    "        \n",
    "        idxs = random.sample(xrange(len(x)), num_samples)\n",
    "        train_features = [x[i] for i in idxs]\n",
    "        labels = [y[i] for i in idxs]\n",
    "    \n",
    "        sweep_end = False\n",
    "        \n",
    "        num_seq = len(train_features)\n",
    "        f_sample_count = num_seq\n",
    "        l_sample_count = num_seq\n",
    "        \n",
    "        \n",
    "        dummyvar = sequence.input(shape=input_dim, is_sparse=False)        \n",
    "        f_data = self.turn_var_into_value_wtf(dummyvar,train_features)\n",
    "        l_data = C.Value(batch=np.asarray(labels, dtype=np.float32))\n",
    "\n",
    "        \n",
    "        result = {\n",
    "#                 self.fsi: train_features,\n",
    "#                 self.lsi: labels\n",
    "                self.fsi: MinibatchData(f_data, num_seq, f_sample_count, sweep_end),\n",
    "                self.lsi: MinibatchData(l_data, num_seq, l_sample_count, sweep_end)\n",
    "               \n",
    "                }\n",
    "\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gettestrnn():\n",
    "    #factor = C.constant([)\n",
    "    def testrnn(prev_h,prev_s,x):\n",
    "            #dhs = Sdh(dh)  # previous value, stabilized\n",
    "            #ht = activation (times(x, W) + times(dhs, H) + b)\n",
    "            #h = times(Sht(ht), Wmr) if has_projection else \\\n",
    "            #    ht\n",
    "            #s = C.reduce_mean(x)\n",
    "            x_mean = C.reduce_mean(x)\n",
    "            x_one_if_high = C.greater(x,x_mean)\n",
    "            sum_high = C.reduce_sum(C.element_times(x,x_one_if_high))\n",
    "            count_high = C.reduce_sum(x_one_if_high)\n",
    "            three_quarter_mark = sum_high/count_high\n",
    "            \n",
    "            upper_count = C.reduce_sum(C.greater(x,three_quarter_mark))\n",
    "            s = (prev_s + prev_s + upper_count)/3\n",
    "            #ADD CLIPPING TODO\n",
    "            #h = (x * C.sigmoid(s)) \n",
    "            h = (x * s) \n",
    "            h += ((prev_h/prev_h) - 1)\n",
    "            #h *= 0\n",
    "            #s *= 0\n",
    "            #return Function.NamedOutput(h=h)\n",
    "            return h,s\n",
    "    return BlockFunction('testrnn', '')(testrnn)\n",
    "\n",
    "def gettestrnn2():\n",
    "    #factor = C.constant([)\n",
    "    def testrnn(prev_h,prev_sum,prev_counter,x):\n",
    "            x_mean = C.reduce_mean(x)\n",
    "            x_one_if_high = C.greater(x,x_mean)\n",
    "            sum_high = C.reduce_sum(C.element_times(x,x_one_if_high))\n",
    "            count_high = C.reduce_sum(x_one_if_high)\n",
    "            this_high = C.greater(count_high,0)\n",
    "  \n",
    "            the_sum = C.minus(C.plus(prev_sum,x),x)\n",
    "            s_counter = (prev_counter + count_high - count_high)\n",
    "\n",
    "\n",
    "            h = C.clip(x,0,0) * the_sum * s_counter \n",
    "            h += (C.clip(prev_h,0,0))\n",
    "            \n",
    "                    \n",
    "\n",
    "            if C.equal(s_counter,0):\n",
    "                the_sum += x\n",
    "                if this_high:\n",
    "                    s_counter += 2\n",
    "                    \n",
    "            else:\n",
    "                if this_high:\n",
    "                    the_sum += C.plus(prev_sum,x)\n",
    "                    s_counter += 1\n",
    "                else:\n",
    "                    s_counter -= s_counter\n",
    "                    the_sum += x\n",
    "                    if C.greater(s_counter,4):\n",
    "                        pass\n",
    "                        h += C.element_divide(the_sum,s_counter)\n",
    "            h = C.reshape(h,(1024))\n",
    "#             the_sum = C.reshape(the_sum,(1))\n",
    "#             s_counter = C.reshape(s_counter,(1))\n",
    "            \n",
    "            return h,the_sum,s_counter\n",
    "                       \n",
    "\n",
    "\n",
    "    return BlockFunction('testrnn2', '')(testrnn)\n",
    "\n",
    "from cntk.internal import sanitize_dtype_cntk\n",
    "zero_cons = Constant.scalar(sanitize_dtype_cntk(np.float32), 0.0)\n",
    "\n",
    "def get_mean_rnn():\n",
    "    #factor = C.constant([)\n",
    "    def mean_rnn(mean,sum_r,counter,x):\n",
    "        #sum_r = C.clip(sum_r,0,0) + x\n",
    "        sum_r += x\n",
    "        \n",
    "        count_high = C.reduce_sum(x)\n",
    "        counter = counter + 1 + (count_high - count_high)\n",
    "        #counter += 1\n",
    "        mean = C.clip(mean,0,0)\n",
    "        \n",
    "        if C.ops.equal(C.sequence.future_value(x),zero_cons): #we are at the last slice\n",
    "            mean += (sum_r / counter)\n",
    "        else:\n",
    "            mean += x\n",
    "        \n",
    "        return mean, sum_r, counter\n",
    "                       \n",
    "    return BlockFunction('mean_rnn', '')(mean_rnn)\n",
    "\n",
    "def get_upper_mean_rnn():\n",
    "    #factor = C.constant([)\n",
    "    def upper_mean_rnn(reduced_mean_storage,sum_r,counter,x):\n",
    "        sum_r += C.clip(x,0,0)\n",
    "        count_high = C.reduce_sum(x)\n",
    "        counter = counter + (count_high - count_high)\n",
    "        reduced_mean_storage += C.clip((sum_r / counter),0,0)\n",
    " \n",
    "        \n",
    "        if C.ops.equal(C.sequence.past_value(x),zero_cons): #we are at first slice\n",
    "            reduced_mean_storage += C.reduce_mean(x)\n",
    "        else:\n",
    "            reduced_mean = C.reduce_mean(reduced_mean_storage)\n",
    "            if C.ops.greater(C.reduce_mean(x),reduced_mean):\n",
    "                sum_r += x\n",
    "                counter = counter + 1\n",
    "            if C.ops.equal(C.sequence.future_value(x),zero_cons): #we are at the last slice\n",
    "                reduced_mean_storage -= reduced_mean_storage #reset to zero\n",
    "                reduced_mean_storage += (sum_r / counter)\n",
    "\n",
    "        \n",
    "        return reduced_mean_storage, sum_r, counter\n",
    "                       \n",
    "    return BlockFunction('upper_mean_rnn', '')(upper_mean_rnn)\n",
    "\n",
    "def get_upper_mean_rnn2():\n",
    "    #factor = C.constant([)\n",
    "    def upper_mean_rnn2(mean,sum_r,counter,x):\n",
    "        sum_r += C.clip(x,0,0)\n",
    "        count_high = C.reduce_sum(x)\n",
    "        counter = counter + (count_high - count_high)\n",
    "        mean += C.clip((sum_r / counter),0,0)\n",
    "\n",
    " \n",
    "        \n",
    "        if C.ops.equal(C.sequence.past_value(x),zero_cons): #we are at first slice\n",
    "            mean += x\n",
    "        else:\n",
    "            real_high_count = C.reduce_sum(C.greater(x,mean))\n",
    "            if C.ops.greater(real_high_count,75):\n",
    "                sum_r += x\n",
    "                counter = counter + 1\n",
    "            if C.ops.equal(C.sequence.future_value(x),zero_cons): #we are at the last slice\n",
    "                mean -= mean #reset to zero\n",
    "                mean += (sum_r / counter)\n",
    "\n",
    "        \n",
    "        return mean, sum_r, counter\n",
    "                       \n",
    "    return BlockFunction('upper_mean_rnn2', '')(upper_mean_rnn2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#below function should make last slice a count of slices...\n",
    "#but it kills training, loss never gets low. bad code? cntk issue? who knows...\n",
    "def get_counter_rnn():\n",
    "    #factor = C.constant([)\n",
    "    def counter_rnn(prev_h,counter,x):\n",
    "        \n",
    "        \n",
    "        count_high = C.reduce_sum(x)\n",
    "        counter = counter + 1 + (count_high - count_high)\n",
    "        #counter += 1\n",
    "        h = (prev_h-prev_h)+(x-x)\n",
    "        \n",
    "        if C.ops.equal(C.sequence.future_value(x),zero_cons): #we are at the last slice\n",
    "            h += counter\n",
    "            h = C.reciprocal(h)\n",
    "        else:\n",
    "            h += x\n",
    "        \n",
    "        \n",
    "        return h, counter\n",
    "                       \n",
    "    return BlockFunction('counter_rnn', '')(counter_rnn)\n",
    "\n",
    "\n",
    "def get_scale_rnn():\n",
    "    #factor = C.constant([)\n",
    "    def scale_rnn(prev_h,scale,x):\n",
    "        scale += (prev_h-prev_h)\n",
    "        \n",
    "        if C.ops.equal(C.sequence.past_value(x),zero_cons): #we are at the first slice\n",
    "            scale += x\n",
    "        \n",
    "        h = x*scale\n",
    "        \n",
    "        return h, scale\n",
    "                       \n",
    "    return BlockFunction('scale_rnn', '')(scale_rnn)\n",
    "\n",
    "\n",
    "#tried to modify mgru with some sequence size info\n",
    "#however could never reliably get access to the sequence size in the first place\n",
    "def get_mgru(shape):\n",
    "    '''\n",
    "    Helper to create a recurrent block of type 'LSTM', 'GRU', or RNNUnit.\n",
    "    '''\n",
    "    activation = C.ops.tanh\n",
    "    init = C.initializer.glorot_uniform()\n",
    "    init_bias = 0\n",
    "    has_projection = False\n",
    "    \n",
    "    _INFERRED = (C.InferredDimension,) \n",
    "\n",
    "    shape = C.internal._as_tuple(shape)\n",
    "\n",
    "    cell_shape = shape\n",
    "    if len(shape) != 1 or len(cell_shape) != 1:\n",
    "        raise ValueError(\"%s: shape and cell_shape must be vectors (rank-1 tensors)\" % type)\n",
    "        # otherwise we'd need to fix slicing and Param initializers\n",
    "\n",
    "    stack_axis = -1  # for efficient computation, we stack multiple variables (along the fastest-changing one, to match BS)\n",
    "    # determine stacking dimensions\n",
    "    cell_shape_list = list(cell_shape)\n",
    "    stacked_dim = cell_shape_list[stack_axis]\n",
    "    cell_shape_list[stack_axis] = stacked_dim * 3\n",
    "    cell_shape_stacked = tuple(cell_shape_list)  # patched dims with stack_axis duplicated 4 times\n",
    "    cell_shape_list[stack_axis] = stacked_dim * 2\n",
    "    cell_shape_stacked_H = tuple(cell_shape_list)  # patched dims with stack_axis duplicated 4 times\n",
    "\n",
    "    # parameters\n",
    "    b  = C.Parameter(            cell_shape_stacked,   init=init_bias, name='b')                              # bias\n",
    "    W  = C.Parameter(_INFERRED + cell_shape_stacked,   init=init,      name='W')                              # input\n",
    "    H  = C.Parameter(shape     + cell_shape_stacked_H, init=init,      name='H')                              # hidden-to-hidden\n",
    "    H1 = C.Parameter(shape     + cell_shape,           init=init,      name='H1') # hidden-to-hidden\n",
    "    H2 = C.Parameter(shape,           init=init,      name='H2') # new\n",
    "    Ci = None  # cell-to-hiddden {note: applied elementwise}\n",
    "    Cf = None  # cell-to-hiddden {note: applied elementwise}\n",
    "    Co = None  # cell-to-hiddden {note: applied elementwise}\n",
    "\n",
    "    Wmr = C.Parameter(cell_shape + shape, init=init, name='P') if has_projection else None  # final projection\n",
    "\n",
    "    # each use of a stabilizer layer must get its own instance\n",
    "    Sdh = C.layers.blocks.Stabilizer(enable_self_stabilization=True, name='dh_stabilizer')\n",
    "\n",
    "    # define the model function itself\n",
    "    # general interface for Recurrence():\n",
    "    #   (all previous outputs delayed, input) --> (outputs and state)\n",
    "    # where\n",
    "    #  - the first output is the main output, e.g. 'h' for LSTM\n",
    "    #  - the remaining outputs, if any, are additional state\n",
    "    #  - if for some reason output != state, then output is still fed back and should just be ignored by the recurrent block\n",
    "\n",
    "    \n",
    "    # GRU model function\n",
    "    # in this case:\n",
    "    #   (dh, x) --> (h)\n",
    "    # e.g. https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "    def grun(dh,rec_length_of_seq, x):\n",
    "        \n",
    "        if C.ops.equal(C.sequence.past_value(x),zero_cons): #we are at first slice\n",
    "            rec_length_of_seq += C.reduce_min(x)\n",
    "        \n",
    "        dhs = Sdh(dh)  # previous value, stabilized\n",
    "        # note: input does not get a stabilizer here, user is meant to do that outside\n",
    "\n",
    "        # projected contribution from input(s), hidden, and bias\n",
    "        projx3 = b + C.times(x, W)\n",
    "        projh2  = C.times(dhs, H)\n",
    "\n",
    "        zt_proj = C.slice (projx3, stack_axis, 0*stacked_dim, 1*stacked_dim) + C.slice (projh2, stack_axis, 0*stacked_dim, 1*stacked_dim)\n",
    "        rt_proj = C.slice (projx3, stack_axis, 1*stacked_dim, 2*stacked_dim) + C.slice (projh2, stack_axis, 1*stacked_dim, 2*stacked_dim)\n",
    "        ct_proj = C.slice (projx3, stack_axis, 2*stacked_dim, 3*stacked_dim)\n",
    "\n",
    "        zt = C.sigmoid (zt_proj)        # update gate z(t)\n",
    "\n",
    "        rt = C.sigmoid (rt_proj)        # reset gate r(t)\n",
    "\n",
    "        rs = dhs * rt        # \"cell\" c\n",
    "        ct = activation (ct_proj + times(rs, H1))\n",
    "\n",
    "        \n",
    "        #new\n",
    "        weighted_los = C.sigmoid(times(rec_length_of_seq,H2))\n",
    "        \n",
    "        ht = (1 - zt) * ct + zt * ((1-weighted_los) * dhs) # hidden state ht / output\n",
    "\n",
    "        # for comparison: CUDNN_GRU\n",
    "        # i(t) = sigmoid(W_i x(t) +          R_i h(t-1)  + b_Wi + b_Ru)\n",
    "        # r(t) = sigmoid(W_r x(t) +          R_r h(t-1)  + b_Wr + b_Rr)   --same up to here\n",
    "        # h'(t) =   tanh(W_h x(t) + r(t) .* (R_h h(t-1)) + b_Wh + b_Rh)   --r applied after projection? Would make life easier!\n",
    "        # h(t) = (1 - i(t) .* h'(t)) + i(t) .* h(t-1)                     --TODO: need to confirm bracketing with NVIDIA\n",
    "\n",
    "        h = C.times(Sht(ht), Wmr) if has_projection else \\\n",
    "            ht\n",
    "\n",
    "        # returns the new state as a tuple with names but order matters\n",
    "        #return Function.NamedOutput(h=h)\n",
    "        return h, rec_length_of_seq\n",
    "\n",
    "    # return the corresponding lambda as a CNTK Function\n",
    "    return BlockFunction('grun', 'grun')(grun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_random_data(sample_size, feature_dim, num_classes):\n",
    "#     # Create synthetic data using NumPy.\n",
    "#     Y = np.random.randint(size=(sample_size, 1), low=0, high=num_classes)\n",
    "#     S = np.random.randint(size=(sample_size), low=10, high=30)\n",
    "\n",
    "#     # Make sure that the data is separable\n",
    "#     X = []\n",
    "#     for i in range(sample_size):\n",
    "#         x = (np.random.randn(S[i], feature_dim) + 3) * (Y[i]+1)\n",
    "#         x = x.astype(np.float32)\n",
    "#         X.append(x)\n",
    "#     # converting class 0 into the vector \"1 0 0\",\n",
    "#     # class 1 into vector \"0 1 0\", ...\n",
    "#     class_ind = [Y == class_number for class_number in range(num_classes)]\n",
    "#     Y = np.asarray(np.hstack(class_ind), dtype=np.float32)\n",
    "#     return X, Y\n",
    "\n",
    "\n",
    "# Defines the STM model for classifying sequences\n",
    "def LSTM_sequence_classifier_net(input, num_output_classes, LSTM_dim, cell_dim):\n",
    "    lstm_classifier = Sequential([\n",
    "        #For(range(3),lambda: [Dense(2048, activation=C.leaky_relu), Dropout(0.4)]),\n",
    "        \n",
    "#         Dense(2048, activation=C.leaky_relu),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        #Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        #BatchNormalization(),\n",
    "        \n",
    "        \n",
    "        #Fold(C.reduce_max),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "        \n",
    "        Fold(get_mean_rnn(),initial_state=(0,0,0)),\n",
    "        \n",
    "#         Dropout(0.2),\n",
    "        #Recurrence(get_counter_rnn(),initial_state=(0,0)),  \n",
    "        #Recurrence(get_scale_rnn(),initial_state=(0,0),go_backwards=True),  \n",
    "        #Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        #Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        #Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "        #LayerNormalization(),\n",
    "        #Recurrence(gettestrnn2(),initial_state=(0.0001,0,0)),\n",
    "        #BatchNormalization(),\n",
    "#         Recurrence(get_counter_rnn(),initial_state=(0,0)),\n",
    "#         Recurrence(get_mgru(LSTM_dim),initial_state=(0,1),go_backwards=True),\n",
    "#         Recurrence(get_counter_rnn(),initial_state=(0,0)),\n",
    "#         Fold(get_mgru(LSTM_dim),initial_state=(0,1),go_backwards=True),\n",
    "        #Recurrence(get_counter_rnn(),initial_state=(0,0),go_backwards=True),\n",
    "        \n",
    "        #Fold(get_upper_mean_rnn(),initial_state=(0,0,0),go_backwards=True),\n",
    "        #Fold(get_sum_rnn(),initial_state=(0,0)),\n",
    "#         Fold(C.plus),\n",
    "        #Dropout(0.3),\n",
    "        #Fold(LSTM(LSTM_dim, cell_dim, enable_self_stabilization=True,use_peepholes=True)),\n",
    "#         sequence.last,\n",
    "        #BatchNormalization(),\n",
    "        #Dropout(0.3),\n",
    "        Dense(num_output_classes)])\n",
    "        \n",
    "    return lstm_classifier(input)\n",
    "\n",
    "\n",
    "cell_dim = 100\n",
    "hidden_dim = 100\n",
    "num_output_classes = 2\n",
    "\n",
    "features = sequence.input(shape=input_dim, is_sparse=False)\n",
    "label = input(num_output_classes)\n",
    "\n",
    "#def make_trainer():\n",
    "\n",
    "\n",
    "    # Input variables denoting the features and label data\n",
    "\n",
    "\n",
    "    # Instantiate the sequence classification model\n",
    "model = LSTM_sequence_classifier_net(\n",
    "    features, num_output_classes, hidden_dim, cell_dim)\n",
    "\n",
    "ce = cross_entropy_with_softmax(model, label)\n",
    "#ce = C.squared_error(model, label)\n",
    "#ce = C.binary_cross_entropy(model, label)\n",
    "#pe = classification_error(model, label)\n",
    "\n",
    "lr_per_sample = learning_rate_schedule(0.001, UnitType.sample)\n",
    "m_per_sample = C.momentum_schedule(0.9, UnitType.sample)\n",
    "# Instantiate the trainer object to drive the model training\n",
    "progress_printer = ProgressPrinter(0)\n",
    "#trainer = Trainer(model, (ce, pe),\n",
    "trainer = Trainer(model, ce,\n",
    "                  #sgd(model.parameters, lr=lr_per_sample),\n",
    "                  C.adam(model.parameters, lr_per_sample, m_per_sample),\n",
    "                  progress_printer)\n",
    "#return trainer\n",
    "\n",
    "#def train_trainer(trainer):\n",
    "    # Get minibatches of sequences to train with and perform model training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mbs = MyDataSource(input_dim, num_output_classes)\n",
    "model_inputs_to_streams = {\n",
    "    features: mbs.fsi,\n",
    "    label: mbs.lsi\n",
    "}\n",
    "\n",
    "\n",
    "checkpoint_config=C.train.CheckpointConfig(\"cntk_ts_checkpoint_v3\",)\n",
    "mb_size = C.train.minibatch_size_schedule(40)\n",
    "\n",
    "ts = C.train.training_session(trainer, mbs, mb_size,model_inputs_to_streams,\n",
    "                              #checkpoint_config=checkpoint_config,\n",
    "                              max_samples=80000\n",
    "                              )\n",
    "\n",
    "#for i in range(20000):\n",
    "\n",
    "#    trainer.train_minibatch(get_data_batch())\n",
    "    #model.save('cntk_run1_s1.model')\n",
    "    #trainer.save_checkpoint(\"cntk_run1_s1.dnn\")\n",
    "\n",
    "        \n",
    "\n",
    "    #evaluation_average = float(trainer.previous_minibatch_evaluation_average)\n",
    "    #loss_average = float(trainer.previous_minibatch_loss_average)\n",
    "    #return trainer\n",
    "    #return evaluation_average, loss_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts.train()\n",
    "#trainer = make_trainer()\n",
    "#trainer = trainer.restore_from_checkpoint(\"cntk_run1_s1.dnn\")\n",
    "#print trainer\n",
    "#trainer = train_trainer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     del ts\n",
    "#     del trainer\n",
    "#     del model\n",
    "# except:\n",
    "#     pass\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate per sample: 0.0005\n",
    "#     0.741      0.741      0.612      0.612           500\n",
    "#      1.16       1.37      0.377       0.26          1500\n",
    "#     0.855      0.627      0.358      0.344          3500\n",
    "#      0.72      0.601      0.313      0.273          7500\n",
    "#     0.644      0.574      0.284      0.258         15500\n",
    "#     0.606      0.568      0.269      0.255         31500\n",
    "#      0.59      0.574      0.265       0.26         63500\n",
    "#     0.571      0.552      0.259      0.254        127500\n",
    "#     0.448      0.326      0.201      0.144        255500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = trainer.model\n",
    "# model.save('version1_10000.model')\n",
    "# trainer.save_checkpoint(\"version1_10000.dnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(STAGE1_SAMPLE_SUBMISSION)\n",
    "sols = pd.read_csv(STAGE1_SAMPLE_SUBMISSION_SOLUTIONS)\n",
    "\n",
    "\n",
    "# testx = [np.squeeze(np.load(FEATURE_FOLDER+'%s.npy' % str(id))) for id in df['id'].tolist()]\n",
    "xresnet = [np.squeeze(np.load(FEATURE_FOLDER+'%s.npy' % str(id))) for id in df['id'].tolist()]\n",
    "xvgg = [np.squeeze(np.load('/a/data/mango/featuresvgg19/'+'%s.npy' % str(id))) for id in df['id'].tolist()]\n",
    "\n",
    "testx=[]\n",
    "for r,v in zip(xresnet,xvgg):\n",
    "    v = v.reshape((v.shape[0],-1))\n",
    "    testx.append(np.concatenate((r,v),axis=1))\n",
    "\n",
    "\n",
    "    \n",
    "    #I tried scaling down data and the summing... worked ok actually but not better than\n",
    "    #other strategies\n",
    "# xm = []\n",
    "# for xi in testx:\n",
    "#     xm.append(xi / xi.shape[0])\n",
    "    \n",
    "# testx = xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = C.ops.functions.Function.load(\"version1_10000.model\")\n",
    "model = trainer.model\n",
    "pred_data_iter = grouper(40,testx)\n",
    "preds = []\n",
    "for data in pred_data_iter:\n",
    "    pred = model.forward(list(data))\n",
    "    _,pred = next(pred[1].iteritems())\n",
    "    pred = scipy.delete(pred, 1, 1)\n",
    "    preds.extend(list(np.squeeze(pred)))\n",
    "pred = np.array(preds)\n",
    "# _,pred = next(pred[1].iteritems())\n",
    "# pred = scipy.delete(pred, 1, 1)  # delete second row of A\n",
    "# pred = np.squeeze(pred)\n",
    "#print (pred)\n",
    "#np.savetxt(\"foo1.csv\", pred, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array(sols['cancer'])\n",
    "#y_pred = np.clip(pred, (1e-15), 1 - (1e-15))\n",
    "#print ((-(truth * np.log(y_pred)) - ((1 - truth)*np.log(1 - y_pred))))\n",
    "                             \n",
    "tprs = []\n",
    "fprs = []\n",
    "for threshold in np.arange(-100,100,0.05):\n",
    "    size = float(truth.shape[0])\n",
    "    predcancer = np.zeros_like(pred)\n",
    "    predcancer[pred>threshold] = 1\n",
    "    tpr_num = tpr_den = fpr_num = fpr_den = 0.\n",
    "    for t,p in zip(truth,predcancer):\n",
    "        if t==1 and p == 1:\n",
    "            tpr_num += 1\n",
    "        if t==1:\n",
    "            tpr_den += 1\n",
    "            \n",
    "        if t==0 and p == 1:\n",
    "            fpr_num += 1\n",
    "        if t==0:\n",
    "            fpr_den += 1\n",
    "    tpr = tpr_num/tpr_den\n",
    "    fpr = fpr_num/fpr_den\n",
    "    tprs.append(tpr)\n",
    "    fprs.append(fpr)\n",
    "    #print (round(threshold,2),\":\",tpr,fpr)\n",
    "    \n",
    "#print (pred)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "smax_pred = (C.softmax(pred).eval())\n",
    "print(metrics.log_loss(truth,smax_pred))\n",
    "\n",
    "print(metrics.auc(fprs,tprs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# plt.plot([0,1],[0,1])#straight line\n",
    "# plt.plot(fprs,tprs)\n",
    "# plt.savefig(\"versionT2highmean_100000.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "cPickle.dump((fprs,tprs), open('roc_data_7.pickle', 'wb')) \n",
    "#import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training loss: with high mean\n",
    "#   0.0813     0.0033          0          0         40920\n",
    "\n",
    "#training loss with regular mean\n",
    "#   0.0413   0.000653          0          0         81880\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with high mean:\n",
    "# 9.96477122127\n",
    "# 0.390319771059\n",
    "\n",
    "#with regular mean\n",
    "# 2.19896640348\n",
    "# 0.633818589026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle\n",
    "# (fprs,tprs) = cPickle.load(open('verstionT2normalmean.pickle', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import cPickle\n",
    "\n",
    "(fprs0,tprs0) = cPickle.load(open('roc_data_0.pickle', 'rb'))\n",
    "(fprs1,tprs1) = cPickle.load(open('roc_data_1.pickle', 'rb'))\n",
    "(fprs2,tprs2) = cPickle.load(open('roc_data_2.pickle', 'rb'))\n",
    "(fprs3,tprs3) = cPickle.load(open('roc_data_3.pickle', 'rb'))\n",
    "(fprs4,tprs4) = cPickle.load(open('roc_data_4.pickle', 'rb'))\n",
    "(fprs5,tprs5) = cPickle.load(open('roc_data_5.pickle', 'rb'))\n",
    "(fprs6,tprs6) = cPickle.load(open('roc_data_6.pickle', 'rb'))\n",
    "(fprs7,tprs7) = cPickle.load(open('roc_data_7.pickle', 'rb'))\n",
    "\n",
    "pl.clf()\n",
    "pl.plot(fprs0, tprs0, label='CNN + Mean (area = 0.64)')\n",
    "pl.plot(fprs1, tprs1, label='CNN + RNN (area = 0.52)')\n",
    "pl.plot(fprs2, tprs2, label='CNN + RNN + Mean (area = 0.62)')\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('CNN ROC curve comparison')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.savefig(\"figure2CNN_ROC.png\")\n",
    "pl.show()\n",
    "\n",
    "pl.clf()\n",
    "pl.plot(fprs6, tprs6, label='RNN + Mean (area = 0.61)')\n",
    "pl.plot(fprs7, tprs7, label='RNN (area = 0.52)')\n",
    "pl.plot(fprs0, tprs0, label='CNN + Mean (area = 0.64)')\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('RNN ROC curve comparison')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.savefig(\"figure3RNN_ROC.png\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FOR MODELS ACTUALLY USED IN PAPER:\n",
    "\n",
    "VERSION 0\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        \n",
    "        Fold(get_mean_rnn(),initial_state=(0,0,0)),\n",
    "\n",
    "basic CNN + mean at end\n",
    "\n",
    "loss: 2.09360234358\n",
    "abc:  0.643212641533\n",
    "\n",
    "\n",
    "\n",
    "VERION 1\n",
    "\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Fold(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "\n",
    "basic CNN + two rnn (last of seq taken)\n",
    "\n",
    "3.0623921966\n",
    "0.517668284186\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VERSION 2\n",
    "\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        \n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "        \n",
    "        Fold(get_mean_rnn(),initial_state=(0,0,0)),\n",
    "\n",
    "cnn + two run + mean\n",
    "\n",
    "3.17747431249\n",
    "0.622869229812\n",
    "\n",
    "\n",
    "\n",
    "VERSION 3\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        \n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "        \n",
    "        Recurrence(get_mean_rnn(),initial_state=(0,0,0)),\n",
    "        Fold(get_upper_mean_rnn2(),initial_state=(0,0,0),go_backwards=True),\n",
    "9.49553715024\n",
    "0.59219858156\n",
    "cnn + rnn + highmean (v2)\n",
    "\n",
    "\n",
    "VERSION 4\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        \n",
    "        \n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "        \n",
    "        Recurrence(get_mean_rnn(),initial_state=(0,0,0)),\n",
    "        Fold(get_upper_mean_rnn(),initial_state=(0,0,0),go_backwards=True),\n",
    "nan\n",
    "0.599415204678\n",
    "cnn + rnn + highmean (v1)\n",
    "\n",
    "\n",
    "VERSION 5\n",
    "\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        C.ops.reshape(Dense(4500, activation=C.leaky_relu),(5,30,30)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Convolution((3,3), num_filters=5, pad=True, activation=C.leaky_relu),MaxPooling((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        C.ops.sequence.reduce_max(Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True)),\n",
    "cnn + rnn + max\n",
    "2.93672954327\n",
    "0.626726390444\n",
    "\n",
    "All previous trained 80,000 iterations\n",
    "batch size 40\n",
    "\n",
    "\n",
    "VERSION 6\n",
    "        BatchNormalization(),\n",
    "\t\tRecurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),        \n",
    "        Fold(get_mean_rnn(),initial_state=(0,0,0)),\n",
    "2.77463137799\n",
    "0.613039691427\n",
    "rnn + mean\n",
    "\n",
    "\n",
    "VERSION 7\n",
    "\t\tBatchNormalization(),\n",
    "        Recurrence(LSTM(LSTM_dim, enable_self_stabilization=True)),\n",
    "        Fold(LSTM(LSTM_dim, enable_self_stabilization=True),go_backwards=True),\n",
    "2.69949306237\n",
    "0.52967525196\n",
    "\n",
    "rnn\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
